{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "574277d4",
   "metadata": {},
   "source": [
    "This notebook demonstrates the complete data journey from raw source to analytics-ready platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fcdeec",
   "metadata": {},
   "source": [
    "# Online Retail Data Pipeline - Executive Summary\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "A UK-based online retailer was struggling with **inconsistent data quality** that blocked their analytics team from performing reliable sales analysis. Without a trusted data foundation, business decisions were being made on incomplete or unreliable information.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "**Source:** UCI Machine Learning Repository - Online Retail Dataset  \n",
    "**Scope:** 541,909 transactions from December 2010 to December 2011  \n",
    "**Challenge:** 24.9% missing customer data, invalid prices, duplicates, and complex business rules\n",
    "\n",
    "## Our Solution\n",
    "\n",
    "Built an **automated ETL pipeline** that transforms raw transactional data into an analytics-ready star schema with:\n",
    "\n",
    "- **Built-in data quality checks** and validation rules\n",
    "- **Business logic enforcement** for cancellations, returns, and wholesale orders\n",
    "- **Production-ready patterns** including job tracking and historical profiling\n",
    "- **Star schema optimisation** for fast analytical queries\n",
    "\n",
    "## Key Results\n",
    "\n",
    "- **98.56% data quality pass rate** after cleaning\n",
    "- **6 business constraints** automatically identified and handled\n",
    "- **534,129 clean transactions** ready for analysis\n",
    "- **Complete audit trail** with job tracking and profiling history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display architecture diagram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "display(Markdown(\"## Data Pipeline Architecture\"))\n",
    "display(Image(filename='../docs/architecture_diagram.png', width=900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726efdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dimensional model (star schema)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "display(Markdown(\"## Dimensional Model (Star Schema)\"))\n",
    "display(Image(filename='../docs/online_retail_schema.png', width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2b966",
   "metadata": {},
   "source": [
    "## Key Design Decisions\n",
    "\n",
    "### 1. **Job ID Traceability**\n",
    "- **Problem:** Need to track pipeline runs for debugging and auditing\n",
    "- **Solution:** Unique job IDs (ex. `f719451c`) with comprehensive logging\n",
    "- **Result:** Every run is traceable from raw data to final output\n",
    "\n",
    "### 2. **Modular Python Architecture**\n",
    "- **Problem:** Monolithic scripts are hard to maintain and test\n",
    "- **Solution:** Separate classes for ingestion, profiling, cleaning, and modeling\n",
    "- **Result:** Each component can be developed, tested, and reused independently\n",
    "\n",
    "### 3. **Business Logic First Approach**\n",
    "- **Problem:** Generic cleaning doesn't handle business-specific rules\n",
    "- **Solution:** Implemented 6 specific business constraints:\n",
    "  1. Cancellation handling (9,288 transactions)\n",
    "  2. Missing CustomerID management (135,080 records)  \n",
    "  3. Negative quantity validation (returns vs errors)\n",
    "  4. Invalid price filtering (2,512 records)\n",
    "  5. Extreme quantity flagging (wholesale orders)\n",
    "  6. Missing description handling\n",
    "\n",
    "### 4. **Star Schema for Analytics**\n",
    "- **Problem:** Transactional data is hard to query for business questions\n",
    "- **Solution:** Dimensional modeling with fact and dimension tables\n",
    "- **Result:** Optimised for business intelligence and reporting\n",
    "\n",
    "### 5. **Multiple Output Formats**\n",
    "- **Problem:** Different consumers need different data formats\n",
    "- **Solution:** Generate both SQLite (relational queries) and Parquet (analytical processing)\n",
    "- **Result:** Flexibility for both SQL analysts and data scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick stats from logs\n",
    "stats_data = {\n",
    "    'Metric': ['Initial Data Volume', 'Final Clean Data', 'Data Quality Pass Rate', \n",
    "               'Business Rules Applied', 'Dimensional Model Size'],\n",
    "    'Value': ['541,909 transactions', '534,129 transactions', '98.56%', \n",
    "              '6 constraints', '538,813 total records']\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "display(stats_df.style.hide())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876ccd0",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "## Ingestion Strategy\n",
    "\n",
    "**Challenge:** Reliably acquire data from an external source with potential API failures  \n",
    "**Solution:** Multi-layered approach with fallback mechanisms\n",
    "\n",
    "### Key Features:\n",
    "- **UCI ML Repository API** as primary source\n",
    "- **Local file fallback** for reliability\n",
    "- **Job ID integration** for traceability\n",
    "\n",
    "## Raw Data Assessment\n",
    "\n",
    "**Initial Dataset:** 541,909 transactions across 8 dimensions\n",
    "- Transaction details (InvoiceNo, StockCode, Description)\n",
    "- Quantitative measures (Quantity, UnitPrice)  \n",
    "- Customer information (CustomerID, Country)\n",
    "- Temporal data (InvoiceDate)\n",
    "\n",
    "**Immediate Observations:**\n",
    "- Mixed data types requiring careful handling\n",
    "- Potential data quality issues visible at ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18fc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Raw Data\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "project_root = os.path.abspath(os.path.join('..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "display(Markdown(\"## Step 1: Data Ingestion\"))\n",
    "\n",
    "from src.data_ingestion import DataIngestion\n",
    "\n",
    "ingestion = DataIngestion()\n",
    "raw_df, raw_path = ingestion.fetch_data(save_local=True)\n",
    "\n",
    "print(f\"Raw data loaded: {len(raw_df):,} rows, {len(raw_df.columns)} columns\")\n",
    "print(f\"Saved to: {raw_path}\")\n",
    "\n",
    "# Show sample of raw data\n",
    "display(Markdown(\"### Sample Raw Data:\"))\n",
    "display(raw_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a59a8",
   "metadata": {},
   "source": [
    "# Data Quality Assessment\n",
    "\n",
    "## Comprehensive Data Quality Analysis\n",
    "\n",
    "**Objective:** Systematically identify and quantify data quality issues before transformation  \n",
    "**Approach:** Automated profiling with business context awareness\n",
    "\n",
    "## Critical Findings from Profiling\n",
    "\n",
    "**Major Data Quality Issues Identified:**\n",
    "1. **135,080 missing CustomerIDs** (24.93% of records) - B2B/wholesale transactions\n",
    "2. **5,268 duplicate records** - Potential data loading errors\n",
    "3. **2,517 invalid prices** (≤ 0) - Data entry errors\n",
    "4. **10,624 negative quantities** - Returns and cancellations\n",
    "5. **1,454 missing descriptions** - Product data gaps\n",
    "\n",
    "**Overall Data Quality Score:** 96.85% completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment Display\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def generate_quality_summary(df):\n",
    "    \"\"\"Generate comprehensive data quality summary (standalone version)\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'dataset_overview': {\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2)\n",
    "        },\n",
    "        'column_types': df.dtypes.astype(str).to_dict(),\n",
    "        'completeness': {\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'missing_percentage': (df.isnull().sum() / len(df) * 100).round(2).to_dict(),\n",
    "            'completeness_score': round((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100, 2)\n",
    "        },\n",
    "        'data_quality_issues': {\n",
    "            'duplicate_rows': df.duplicated().sum(),\n",
    "            'negative_quantities': (df['Quantity'] < 0).sum() if 'Quantity' in df.columns else 0,\n",
    "            'zero_quantities': (df['Quantity'] == 0).sum() if 'Quantity' in df.columns else 0,\n",
    "            'non_positive_prices': (df['UnitPrice'] <= 0).sum() if 'UnitPrice' in df.columns else 0,\n",
    "            'zero_prices': (df['UnitPrice'] == 0).sum() if 'UnitPrice' in df.columns else 0,\n",
    "            'missing_customer_ids': df['CustomerID'].isnull().sum() if 'CustomerID' in df.columns else 0,\n",
    "            'missing_descriptions': df['Description'].isnull().sum() if 'Description' in df.columns else 0\n",
    "        },\n",
    "        'business_logic_constraints': identify_business_constraints(df)\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def identify_business_constraints(df):\n",
    "    \"\"\"Identify business logic constraints that need to be applied\"\"\"\n",
    "    constraints = []\n",
    "    \n",
    "    # Rule 1: Cancellations handling\n",
    "    if 'InvoiceNo' in df.columns:\n",
    "        cancellation_count = df['InvoiceNo'].astype(str).str.startswith('C').sum()\n",
    "        constraints.append({\n",
    "            'constraint': 'Cancellation transactions',\n",
    "            'count': int(cancellation_count),\n",
    "            'action_needed': 'Flag as cancellations but keep for refund analysis'\n",
    "        })\n",
    "    \n",
    "    # Rule 2: Missing CustomerIDs\n",
    "    if 'CustomerID' in df.columns:\n",
    "        missing_customers = df['CustomerID'].isnull().sum()\n",
    "        constraints.append({\n",
    "            'constraint': 'Missing CustomerIDs',\n",
    "            'count': int(missing_customers),\n",
    "            'percentage': round((missing_customers / len(df)) * 100, 2),\n",
    "            'action_needed': 'Assign surrogate key (0) for unknown customers'\n",
    "        })\n",
    "    \n",
    "    # Rule 3: Negative Quantities\n",
    "    if 'Quantity' in df.columns:\n",
    "        negative_qty = (df['Quantity'] < 0).sum()\n",
    "        constraints.append({\n",
    "            'constraint': 'Negative Quantities',\n",
    "            'count': int(negative_qty),\n",
    "            'action_needed': 'Validate against cancellation flag; keep legitimate returns'\n",
    "        })\n",
    "    \n",
    "    # Rule 4: Invalid Prices\n",
    "    if 'UnitPrice' in df.columns:\n",
    "        invalid_prices = (df['UnitPrice'] <= 0).sum()\n",
    "        constraints.append({\n",
    "            'constraint': 'Invalid Prices (≤ 0)',\n",
    "            'count': int(invalid_prices),\n",
    "            'action_needed': 'Exclude from fact table as they represent data errors'\n",
    "        })\n",
    "    \n",
    "    # Rule 5: Extreme Quantities\n",
    "    if 'Quantity' in df.columns:\n",
    "        high_quantities = (df['Quantity'].abs() > 10000).sum()\n",
    "        constraints.append({\n",
    "            'constraint': 'Extreme Quantities',\n",
    "            'count': int(high_quantities),\n",
    "            'action_needed': 'Flag for business review but keep for wholesale analysis'\n",
    "        })\n",
    "    \n",
    "    # Rule 6: Missing Descriptions\n",
    "    if 'Description' in df.columns:\n",
    "        missing_desc = df['Description'].isnull().sum()\n",
    "        if missing_desc > 0:\n",
    "            constraints.append({\n",
    "                'constraint': 'Missing Product Descriptions',\n",
    "                'count': int(missing_desc),\n",
    "                'action_needed': 'Fill with \"Unknown Product\" placeholder'\n",
    "            })\n",
    "    \n",
    "    return constraints\n",
    "\n",
    "# Generate quality assessment\n",
    "try:\n",
    "    # Use the raw_df from previous cells\n",
    "    if 'raw_df' in locals():\n",
    "        quality_summary = generate_quality_summary(raw_df)\n",
    "        \n",
    "        display(Markdown(f\"### Data Quality Scorecard\"))\n",
    "        \n",
    "        # Overall Quality Metrics\n",
    "        overall_metrics = {\n",
    "            'Metric': [\n",
    "                'Data Completeness Score',\n",
    "                'Total Data Quality Issues', \n",
    "                'Records with Quality Flags',\n",
    "                'Business Constraints Identified',\n",
    "                'Data Quality Pass Rate'\n",
    "            ],\n",
    "            'Value': [\n",
    "                f\"{quality_summary['completeness']['completeness_score']}%\",\n",
    "                f\"{sum(quality_summary['data_quality_issues'].values()):,}\",\n",
    "                f\"{(sum(quality_summary['data_quality_issues'].values()) / quality_summary['dataset_overview']['row_count'] * 100):.1f}% of records\",\n",
    "                f\"{len(quality_summary['business_logic_constraints'])} rules\",\n",
    "                '98.56% (after cleaning)'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame(overall_metrics)\n",
    "        display(metrics_df.style.hide(axis='index'))\n",
    "        \n",
    "        # Data Quality Issues Breakdown\n",
    "        display(Markdown(\"### Data Quality Issues Breakdown\"))\n",
    "        \n",
    "        issues_data = quality_summary['data_quality_issues']\n",
    "        issues_df = pd.DataFrame({\n",
    "            'Issue Type': [key.replace('_', ' ').title() for key in issues_data.keys()],\n",
    "            'Count': list(issues_data.values()),\n",
    "            'Percentage': [f\"{(count / quality_summary['dataset_overview']['row_count'] * 100):.2f}%\" \n",
    "                          for count in issues_data.values()]\n",
    "        })\n",
    "        \n",
    "        # Sort by count descending\n",
    "        issues_df = issues_df.sort_values('Count', ascending=False)\n",
    "        display(issues_df.style.hide(axis='index'))\n",
    "         \n",
    "    else:\n",
    "        display(Markdown(\"### No raw data available\"))\n",
    "        display(Markdown(\"Please run Cell 1 first to load the raw data.\"))\n",
    "        \n",
    "except Exception as e:\n",
    "    display(Markdown(\"### Error in quality assessment\"))\n",
    "    display(Markdown(f\"Error details: {str(e)}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Data Profiling\n",
    "display(Markdown(\"## Interactive Data Profiling\"))\n",
    "\n",
    "# Basic dataset overview\n",
    "display(Markdown(\"### Dataset Overview\"))\n",
    "print(f\"Shape: {raw_df.shape}\")\n",
    "print(f\"Memory usage: {raw_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Data types\n",
    "display(Markdown(\"### Data Types\"))\n",
    "type_summary = pd.DataFrame({\n",
    "    'Column': raw_df.columns,\n",
    "    'Data Type': raw_df.dtypes,\n",
    "    'Non-Null Count': raw_df.count(),\n",
    "    'Null Count': raw_df.isnull().sum(),\n",
    "    'Null Percentage': (raw_df.isnull().sum() / len(raw_df) * 100).round(2)\n",
    "})\n",
    "display(type_summary.style.hide(axis='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Statistical Summary\n",
    "display(Markdown(\"## Statistical Summary\"))\n",
    "\n",
    "# Numerical columns summary\n",
    "numerical_cols = ['Quantity', 'UnitPrice']\n",
    "display(Markdown(\"### Numerical Columns Summary\"))\n",
    "display(raw_df[numerical_cols].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Latest Profile Report\n",
    "display(Markdown(\"## Latest Profiling Report\"))\n",
    "\n",
    "import glob\n",
    "\n",
    "# Find the latest profile report\n",
    "report_files = glob.glob(os.path.join(project_root, 'data', 'profiling', 'reports', 'profile_report_*.txt'))\n",
    "if report_files:\n",
    "    latest_report = sorted(report_files)[-1]\n",
    "    \n",
    "    # Display report info\n",
    "    display(Markdown(f\"### Latest Report: `{os.path.basename(latest_report)}`\"))\n",
    "    \n",
    "    # Read and display the report content\n",
    "    with open(latest_report, 'r') as f:\n",
    "        report_content = f.read()\n",
    "    \n",
    "    # Display in a scrollable text area\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(f\"<textarea rows='25' cols='120' readonly style='font-family: monospace; font-size: 12px;'>{report_content}</textarea>\"))\n",
    "    \n",
    "else:\n",
    "    display(Markdown(\"### No Profile Reports Found\"))\n",
    "    print(\"Run the profiling pipeline first to generate reports.\")\n",
    "    print(\"Execute: python src/main_pipeline.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d189d79",
   "metadata": {},
   "source": [
    "# Data Cleaning & Business Logic Implementation\n",
    "\n",
    "## From Raw Data to Trusted Analytics\n",
    "\n",
    "**Objective:** Systematically apply business rules to transform raw data into clean, reliable datasets  \n",
    "**Approach:** Rule-based cleaning with domain-specific logic preservation\n",
    "\n",
    "### Cleaning Philosophy:\n",
    "- **Preserve Business Context:** Don't just remove \"bad\" data. Understand and handle it appropriately\n",
    "- **Graceful Degradation:** Handle edge cases without breaking the pipeline\n",
    "- **Audit Trail:** Maintain records of what was changed and why\n",
    "\n",
    "## Applied Business Logic Rules\n",
    "\n",
    "**6 Critical Business Rules Implemented:**\n",
    "\n",
    "1. **Cancellation Handling** (9,288 transactions) - Flag but preserve for refund analysis\n",
    "2. **Missing CustomerID Management** (135,080 records) - Assign surrogate keys for B2B/wholesale\n",
    "3. **Negative Quantity Validation** (10,624 records) - Distinguish returns from data errors  \n",
    "4. **Invalid Price Filtering** (2,512 records) - Remove data entry errors\n",
    "5. **Extreme Quantity Flagging** (4 records) - Identify wholesale orders for review\n",
    "6. **Missing Description Handling** (1,454 records) - Standardize product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Visualisation\n",
    "display(Markdown(\"### Missing Values Analysis\"))\n",
    "\n",
    "# Calculate missing values\n",
    "missing_data = raw_df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(raw_df)) * 100\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot of missing values\n",
    "missing_data[missing_data > 0].plot(kind='bar', ax=ax1, color='coral')\n",
    "ax1.set_title('Missing Values by Column')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Percentage plot\n",
    "missing_percentage[missing_percentage > 0].plot(kind='bar', ax=ax2, color='lightcoral')\n",
    "ax2.set_title('Missing Values Percentage')\n",
    "ax2.set_ylabel('Percentage (%)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display missing values table\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).round(2)\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f74bbb",
   "metadata": {},
   "source": [
    "# Dimensional Modeling & Star Schema Design\n",
    "\n",
    "## Building an Analytics-Optimised Database\n",
    "\n",
    "**Objective:** Transform cleaned transactional data into a star schema optimised for business intelligence  \n",
    "**Approach:** Kimball methodology with fact and dimension tables for fast, intuitive queries\n",
    "\n",
    "### Star Schema Benefits:\n",
    "- **Query Performance:** Denormalised structure for fast aggregations\n",
    "- **Scalability:** Easy to extend with new dimensions and facts\n",
    "- **Maintainability:** Clear separation of dimensions and measures\n",
    "\n",
    "## Schema Architecture\n",
    "\n",
    "**Fact Table:** `fact_sales` - 534,129 transaction records with measures\n",
    "- **Measures:** Quantity, UnitPrice, LineTotal\n",
    "- **Foreign Keys:** Date, Product, Customer\n",
    "- **Flags:** Cancellations, High quantity alerts\n",
    "\n",
    "**Dimension Tables:**\n",
    "- `dim_date` - 374 date records with hierarchical attributes\n",
    "- `dim_product` - 3,938 products with descriptions and lifecycle\n",
    "- `dim_customer` - 4,372 customers with geographic and behavioral attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensional Modeling Implementation & Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def demonstrate_dimensional_model(cleaned_df):\n",
    "    \"\"\"Demonstrate the dimensional modeling process and results\"\"\"\n",
    "    \n",
    "    display(Markdown(\"### Dimensional Modeling Process\"))\n",
    "    \n",
    "    modeling_steps = [\n",
    "        {\n",
    "            'step': 'Date Dimension Creation',\n",
    "            'description': 'Generate complete date range with hierarchical attributes',\n",
    "            'business_value': 'Enables time intelligence (YTD, QoQ, MoM analysis)'\n",
    "        },\n",
    "        {\n",
    "            'step': 'Product Dimension Creation', \n",
    "            'description': 'Deduplicate products and track lifecycle dates',\n",
    "            'business_value': 'Clean product master for categorization and trending'\n",
    "        },\n",
    "        {\n",
    "            'step': 'Customer Dimension Creation',\n",
    "            'description': 'Standardize customer information with geographic context',\n",
    "            'business_value': 'Customer segmentation and geographic analysis'\n",
    "        },\n",
    "        {\n",
    "            'step': 'Fact Table Assembly',\n",
    "            'description': 'Join transactions with dimension keys and calculate measures',\n",
    "            'business_value': 'Single source of truth for all sales metrics'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    steps_df = pd.DataFrame(modeling_steps)\n",
    "    display(steps_df.style.hide(axis='index'))\n",
    "    \n",
    "    # Check if we have the modeled data available\n",
    "    db_path = '../data/model/retail_analytics.db'\n",
    "    \n",
    "    if os.path.exists(db_path):\n",
    "        display(Markdown(\"### Star Schema Analysis\"))\n",
    "        \n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Get table statistics\n",
    "        table_stats = []\n",
    "        tables = ['dim_date', 'dim_product', 'dim_customer', 'fact_sales']\n",
    "        \n",
    "        for table in tables:\n",
    "            count = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0]['count']\n",
    "            table_stats.append({\n",
    "                'Table': table,\n",
    "                'Records': f\"{count:,}\",\n",
    "                'Type': 'Dimension' if table.startswith('dim_') else 'Fact',\n",
    "                'Primary Key': 'date_key' if table == 'dim_date' else \n",
    "                              'product_key' if table == 'dim_product' else \n",
    "                              'customer_key' if table == 'dim_customer' else 'transaction_key'\n",
    "            })\n",
    "        \n",
    "        stats_df = pd.DataFrame(table_stats)\n",
    "        display(stats_df.style.hide(axis='index'))\n",
    "        \n",
    "        # Schema Visualization\n",
    "        display(Markdown(\"### Schema Structure\"))\n",
    "        \n",
    "        # Show sample data from each table\n",
    "        display(Markdown(\"#### Date Dimension Sample\"))\n",
    "        date_sample = pd.read_sql(\"SELECT * FROM dim_date LIMIT 5\", conn)\n",
    "        display(date_sample.style.hide(axis='index'))\n",
    "        \n",
    "        display(Markdown(\"#### Product Dimension Sample\")) \n",
    "        product_sample = pd.read_sql(\"SELECT product_key, stock_code, description, is_active FROM dim_product LIMIT 5\", conn)\n",
    "        display(product_sample.style.hide(axis='index'))\n",
    "        \n",
    "        display(Markdown(\"#### Customer Dimension Sample\"))\n",
    "        customer_sample = pd.read_sql(\"SELECT customer_key, customer_id, country, is_unknown_customer FROM dim_customer LIMIT 5\", conn)\n",
    "        display(customer_sample.style.hide(axis='index'))\n",
    "        \n",
    "        display(Markdown(\"#### Fact Table Sample\"))\n",
    "        fact_sample = pd.read_sql(\"\"\"\n",
    "            SELECT transaction_key, date_key, product_key, customer_key, \n",
    "                   quantity, unit_price, line_total, is_cancelled \n",
    "            FROM fact_sales \n",
    "            LIMIT 5\n",
    "        \"\"\", conn)\n",
    "        display(fact_sample.style.hide(axis='index'))      \n",
    "           \n",
    "        # Analytical Capabilities Demonstration\n",
    "        display(Markdown(\"### Analytical Capabilities Enabled\"))\n",
    "        \n",
    "        # Example 1: Monthly Revenue Trend\n",
    "        display(Markdown(\"#### Example 1: Monthly Revenue Trend\"))\n",
    "        revenue_trend = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                d.year,\n",
    "                d.month,\n",
    "                d.month_name,\n",
    "                SUM(f.line_total) as monthly_revenue,\n",
    "                COUNT(*) as transaction_count\n",
    "            FROM fact_sales f\n",
    "            JOIN dim_date d ON f.date_key = d.date_key\n",
    "            WHERE NOT f.is_cancelled\n",
    "            GROUP BY d.year, d.month, d.month_name\n",
    "            ORDER BY d.year, d.month\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        display(revenue_trend.style.hide(axis='index'))\n",
    "        \n",
    "        # Example 2: Top Products by Revenue\n",
    "        display(Markdown(\"#### Example 2: Top 10 Products by Revenue\"))\n",
    "        top_products = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                p.description,\n",
    "                SUM(f.line_total) as total_revenue,\n",
    "                SUM(f.quantity) as total_quantity,\n",
    "                COUNT(*) as transaction_count\n",
    "            FROM fact_sales f\n",
    "            JOIN dim_product p ON f.product_key = p.product_key\n",
    "            WHERE NOT f.is_cancelled\n",
    "            GROUP BY p.description\n",
    "            ORDER BY total_revenue DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        display(top_products.style.hide(axis='index'))\n",
    "        \n",
    "        # Example 3: Customer Geographic Analysis\n",
    "        display(Markdown(\"#### Example 3: Revenue by Country\"))\n",
    "        country_revenue = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                c.country,\n",
    "                SUM(f.line_total) as total_revenue,\n",
    "                COUNT(DISTINCT c.customer_key) as unique_customers,\n",
    "                COUNT(*) as transaction_count\n",
    "            FROM fact_sales f\n",
    "            JOIN dim_customer c ON f.customer_key = c.customer_key\n",
    "            WHERE NOT f.is_cancelled\n",
    "            GROUP BY c.country\n",
    "            ORDER BY total_revenue DESC\n",
    "            LIMIT 10\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        display(country_revenue.style.hide(axis='index'))\n",
    "        \n",
    "        # Performance Benefits\n",
    "        display(Markdown(\"### Performance & Usability Benefits\"))\n",
    "        \n",
    "        benefits = {\n",
    "            'Benefit': [\n",
    "                'Query Performance',\n",
    "                'Business Understanding',\n",
    "                'Data Consistency',\n",
    "                'Scalability',\n",
    "                'Maintainability'\n",
    "            ],\n",
    "            'Technical Impact': [\n",
    "                'Fast aggregations with pre-joined dimensions',\n",
    "                'Intuitive table structure aligned with business concepts',\n",
    "                'Single source of truth for metrics and dimensions',\n",
    "                'Easy to add new dimensions without schema changes',\n",
    "                'Clear separation of concerns between teams'\n",
    "            ],\n",
    "            'Business Value': [\n",
    "                'Faster reporting and dashboard performance',\n",
    "                'Reduced training time for business users',\n",
    "                'Consistent metrics across all reports',\n",
    "                'Flexible to evolving business needs',\n",
    "                'Lower maintenance costs'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        benefits_df = pd.DataFrame(benefits)\n",
    "        display(benefits_df.style.hide(axis='index'))\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    else:\n",
    "        display(Markdown(\"### Model database not found\"))\n",
    "        display(Markdown(\"Please run the complete pipeline to generate the dimensional model.\"))\n",
    "        \n",
    "        # Fallback: Show what the modeling would create\n",
    "        display(Markdown(\"#### Planned Schema Structure\"))\n",
    "        \n",
    "        schema_overview = {\n",
    "            'Table': ['fact_sales', 'dim_date', 'dim_product', 'dim_customer'],\n",
    "            'Records': ['534,129', '374', '3,938', '4,372'],\n",
    "            'Key Columns': [\n",
    "                'transaction_key, date_key, product_key, customer_key, quantity, unit_price, line_total',\n",
    "                'date_key, full_date, year, quarter, month, day, day_of_week',\n",
    "                'product_key, stock_code, description, first_seen_date, last_seen_date',\n",
    "                'customer_key, customer_id, country, first_purchase_date, last_purchase_date'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        schema_df = pd.DataFrame(schema_overview)\n",
    "        display(schema_df.style.hide(axis='index'))\n",
    "\n",
    "# Execute dimensional modeling demonstration\n",
    "try:\n",
    "    if 'cleaned_df' in locals():\n",
    "        demonstrate_dimensional_model(cleaned_df)\n",
    "    else:\n",
    "        display(Markdown(\"### ❌ Cleaned data not available\"))\n",
    "        display(Markdown(\"Please run the cleaning process in previous cells first.\"))\n",
    "        \n",
    "except Exception as e:\n",
    "    display(Markdown(\"### ❌ Error in dimensional modeling demonstration\"))\n",
    "    display(Markdown(f\"Error details: {str(e)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44afb74b",
   "metadata": {},
   "source": [
    "# Pipeline Performance and Historical Trends\n",
    "\n",
    "## Monitoring Data Quality Evolution\n",
    "\n",
    "**Objective:** Track pipeline performance and data quality metrics over time to identify trends and improvements  \n",
    "**Approach:** Comprehensive profiling history with trend analysis and performance monitoring\n",
    "\n",
    "## Historical Analysis Benefits\n",
    "\n",
    "**Proactive Monitoring:** Identify degradation before it impacts business users  \n",
    "**Continuous Improvement:** Measure the impact of pipeline enhancements\n",
    "**Audit Compliance:** Maintain complete history of data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a565fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Performance and Historical Trends Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "def analyse_pipeline_history():\n",
    "    \"\"\"Analyze pipeline performance and data quality trends over time\"\"\"\n",
    "    \n",
    "    display(Markdown(\"### Pipeline Execution History\"))\n",
    "    \n",
    "    # Try multiple possible paths\n",
    "    possible_paths = [\n",
    "        'data/profiling/profiling_history.csv',  # Relative path from notebook\n",
    "        '/data/profiling/profiling_history.csv',  # Absolute path you mentioned\n",
    "        '../data/profiling/profiling_history.csv',  # If notebook is in docs folder\n",
    "        '../profiling/profiling_history.csv'  # If profiling is in root\n",
    "    ]\n",
    "    \n",
    "    history_df = None\n",
    "    used_path = None\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            history_df = pd.read_csv(path)\n",
    "            used_path = path\n",
    "            display(Markdown(f\"Found profiling history at: `{path}`\"))\n",
    "            break\n",
    "    \n",
    "    if history_df is None:\n",
    "        display(Markdown(\"### No profiling history found at any expected location\"))\n",
    "        display(Markdown(\"**Tried paths:**\"))\n",
    "        for path in possible_paths:\n",
    "            display(Markdown(f\"- `{path}`\"))\n",
    "        display(Markdown(\"\\n**To fix:**\"))\n",
    "        display(Markdown(\"1. Run the pipeline to generate profiling data\"))\n",
    "        display(Markdown(\"2. Check the actual file location\"))\n",
    "        display(Markdown(\"3. Update the path in this cell if needed\"))\n",
    "        return\n",
    "    \n",
    "    # Process the data\n",
    "    history_df['run_timestamp'] = pd.to_datetime(history_df['run_timestamp'])\n",
    "    history_df = history_df.sort_values('run_timestamp')\n",
    "    \n",
    "    display(Markdown(f\"#### Historical Overview: {len(history_df)} Pipeline Runs\"))\n",
    "    \n",
    "    # Basic history stats\n",
    "    history_stats = {\n",
    "        'Metric': [\n",
    "            'Total Pipeline Runs',\n",
    "            'Date Range Covered',\n",
    "            'Average Data Volume',\n",
    "            'Average Completeness Score',\n",
    "            'Most Recent Job ID'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{len(history_df)}\",\n",
    "            f\"{history_df['run_timestamp'].min().strftime('%Y-%m-%d %H:%M')} to {history_df['run_timestamp'].max().strftime('%Y-%m-%d %H:%M')}\",\n",
    "            f\"{history_df['total_rows'].mean():,.0f} records\",\n",
    "            f\"{history_df['completeness_score'].mean():.1f}%\",\n",
    "            f\"{history_df.iloc[-1]['job_id']}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    stats_df = pd.DataFrame(history_stats)\n",
    "    display(stats_df.style.hide(axis='index'))\n",
    "    \n",
    "    # Display recent runs\n",
    "    display(Markdown(\"#### Recent Pipeline Executions\"))\n",
    "    recent_runs = history_df.tail(5)[['job_id', 'run_timestamp', 'total_rows', 'completeness_score', 'duplicate_rows']].copy()\n",
    "    recent_runs['run_timestamp'] = recent_runs['run_timestamp'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    recent_runs.columns = ['Job ID', 'Run Time', 'Total Rows', 'Completeness %', 'Duplicates']\n",
    "    display(recent_runs.style.hide(axis='index'))       \n",
    "\n",
    "# Execute pipeline performance analysis\n",
    "try:\n",
    "    analyse_pipeline_history()\n",
    "except Exception as e:\n",
    "    display(Markdown(\"### Error in pipeline performance analysis\"))\n",
    "    display(Markdown(f\"Error details: {str(e)}\"))\n",
    "    \n",
    "    # Debug: Show current directory and files\n",
    "    display(Markdown(\"#### Debug Information\"))\n",
    "    display(Markdown(f\"**Current working directory:** `{os.getcwd()}`\"))\n",
    "    display(Markdown(\"**Files in current directory:**\"))\n",
    "    try:\n",
    "        files = os.listdir('.')\n",
    "        for file in files:\n",
    "            display(Markdown(f\"- `{file}`\"))\n",
    "    except:\n",
    "        display(Markdown(\"Could not list directory contents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187dfad6",
   "metadata": {},
   "source": [
    "# Analytical Queries and Business Insights\n",
    "\n",
    "## Transforming Data into Business Value\n",
    "\n",
    "**Objective:** Demonstrate how the cleaned, modeled data enables powerful business analytics  \n",
    "**Approach:** Real-world business questions answered through SQL queries on the star schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Queries and Business Insights\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def demonstrate_business_insights():\n",
    "    \"\"\"Demonstrate business insights enabled by the star schema\"\"\"\n",
    "    \n",
    "    display(Markdown(\"### Business Intelligence Summary\"))\n",
    "    \n",
    "    # Try multiple possible database paths\n",
    "    possible_db_paths = [\n",
    "        'data/model/retail_analytics.db',  # Relative path\n",
    "        '/data/model/retail_analytics.db',  # Absolute path you mentioned\n",
    "        '../data/model/retail_analytics.db'  # If notebook is in docs folder\n",
    "    ]\n",
    "    \n",
    "    conn = None\n",
    "    used_path = None\n",
    "    \n",
    "    for db_path in possible_db_paths:\n",
    "        if os.path.exists(db_path):\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            used_path = db_path\n",
    "            display(Markdown(f\"Connected to database at: `{db_path}`\"))\n",
    "            break\n",
    "    \n",
    "    if conn is None:\n",
    "        display(Markdown(\"### Database not found at any expected location\"))\n",
    "        display(Markdown(\"**Tried paths:**\"))\n",
    "        for path in possible_db_paths:\n",
    "            display(Markdown(f\"- `{path}`\"))\n",
    "        display(Markdown(\"\\n**Available files in data/model/:**\"))\n",
    "        try:\n",
    "            model_files = os.listdir('data/model')\n",
    "            for file in model_files:\n",
    "                display(Markdown(f\"- `{file}`\"))\n",
    "        except:\n",
    "            display(Markdown(\"Could not list model directory\"))\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Continue with the rest of the analysis...\n",
    "        # Insight 1: Overall Business Performance\n",
    "        display(Markdown(\"#### Overall Business Performance\"))\n",
    "        \n",
    "        performance_metrics = pd.read_sql(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_transactions,\n",
    "                SUM(CASE WHEN NOT is_cancelled THEN 1 ELSE 0 END) as successful_transactions,\n",
    "                SUM(CASE WHEN is_cancelled THEN 1 ELSE 0 END) as cancelled_transactions,\n",
    "                SUM(CASE WHEN NOT is_cancelled THEN line_total ELSE 0 END) as total_revenue,\n",
    "                AVG(CASE WHEN NOT is_cancelled THEN line_total ELSE NULL END) as avg_transaction_value,\n",
    "                COUNT(DISTINCT customer_key) as unique_customers,\n",
    "                COUNT(DISTINCT product_key) as unique_products\n",
    "            FROM fact_sales\n",
    "        \"\"\", conn)\n",
    "        \n",
    "        perf_data = {\n",
    "            'Metric': [\n",
    "                'Total Transactions',\n",
    "                'Successful Transactions', \n",
    "                'Cancellation Rate',\n",
    "                'Total Revenue',\n",
    "                'Average Transaction Value',\n",
    "                'Unique Customers',\n",
    "                'Unique Products'\n",
    "            ],\n",
    "            'Value': [\n",
    "                f\"{performance_metrics.iloc[0]['total_transactions']:,}\",\n",
    "                f\"{performance_metrics.iloc[0]['successful_transactions']:,}\",\n",
    "                f\"{(performance_metrics.iloc[0]['cancelled_transactions'] / performance_metrics.iloc[0]['total_transactions'] * 100):.1f}%\",\n",
    "                f\"£{performance_metrics.iloc[0]['total_revenue']:,.2f}\",\n",
    "                f\"£{performance_metrics.iloc[0]['avg_transaction_value']:.2f}\",\n",
    "                f\"{performance_metrics.iloc[0]['unique_customers']:,}\",\n",
    "                f\"{performance_metrics.iloc[0]['unique_products']:,}\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        perf_df = pd.DataFrame(perf_data)\n",
    "        display(perf_df.style.hide(axis='index'))\n",
    "        \n",
    "        # Continue with the rest of the insights...\n",
    "        # [Rest of your analytical queries and visualizations]\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(Markdown(\"### Error in business insights analysis\"))\n",
    "        display(Markdown(f\"Error details: {str(e)}\"))\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Execute business insights demonstration\n",
    "try:\n",
    "    demonstrate_business_insights()\n",
    "except Exception as e:\n",
    "    display(Markdown(\"### Error in business insights analysis\"))\n",
    "    display(Markdown(f\"Error details: {str(e)}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
